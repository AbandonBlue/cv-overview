{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T00:11:24.992309Z",
     "start_time": "2020-06-17T00:11:24.986326Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "DN8pv2fDvMhA"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T00:11:55.455845Z",
     "start_time": "2020-06-17T00:11:55.447866Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fYmwZwRKvMhI",
    "outputId": "6e8a04e2-e8d3-4552-a128-014c143300a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T00:28:17.454144Z",
     "start_time": "2020-06-17T00:28:17.446166Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "9OBrmIh0vMhO"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Flatten, Input, GlobalAveragePooling2D, BatchNormalization\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MY62KUovvMhU"
   },
   "source": [
    "## 資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T00:40:09.942129Z",
     "start_time": "2020-06-17T00:40:09.328772Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "colab_type": "code",
    "id": "eb2tsaPZvMhV",
    "outputId": "c94af42d-d3ce-446a-87c3-cf227cdfa3c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 3s 0us/step\n",
      "(50000, 32, 32, 3)\n",
      "(10000, 32, 32, 3)\n",
      "(50000, 1)\n",
      "(10000, 1)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T00:39:05.033194Z",
     "start_time": "2020-06-17T00:39:05.027209Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 857
    },
    "colab_type": "code",
    "id": "dRsoedHTvMhb",
    "outputId": "cc0db4f1-4857-4493-f85a-4820ba48b262",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 59,  62,  63],\n",
       "        [ 43,  46,  45],\n",
       "        [ 50,  48,  43],\n",
       "        ...,\n",
       "        [158, 132, 108],\n",
       "        [152, 125, 102],\n",
       "        [148, 124, 103]],\n",
       "\n",
       "       [[ 16,  20,  20],\n",
       "        [  0,   0,   0],\n",
       "        [ 18,   8,   0],\n",
       "        ...,\n",
       "        [123,  88,  55],\n",
       "        [119,  83,  50],\n",
       "        [122,  87,  57]],\n",
       "\n",
       "       [[ 25,  24,  21],\n",
       "        [ 16,   7,   0],\n",
       "        [ 49,  27,   8],\n",
       "        ...,\n",
       "        [118,  84,  50],\n",
       "        [120,  84,  50],\n",
       "        [109,  73,  42]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[208, 170,  96],\n",
       "        [201, 153,  34],\n",
       "        [198, 161,  26],\n",
       "        ...,\n",
       "        [160, 133,  70],\n",
       "        [ 56,  31,   7],\n",
       "        [ 53,  34,  20]],\n",
       "\n",
       "       [[180, 139,  96],\n",
       "        [173, 123,  42],\n",
       "        [186, 144,  30],\n",
       "        ...,\n",
       "        [184, 148,  94],\n",
       "        [ 97,  62,  34],\n",
       "        [ 83,  53,  34]],\n",
       "\n",
       "       [[177, 144, 116],\n",
       "        [168, 129,  94],\n",
       "        [179, 142,  87],\n",
       "        ...,\n",
       "        [216, 184, 140],\n",
       "        [151, 118,  84],\n",
       "        [123,  92,  72]]], dtype=uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BvMuwfEpvMhi"
   },
   "source": [
    "## 前處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T00:40:29.515887Z",
     "start_time": "2020-06-17T00:40:28.486676Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "eY0LrEHavMhi"
   },
   "outputs": [],
   "source": [
    "# 歸一化\n",
    "\n",
    "x_train = (x_train - 255.) / 255.\n",
    "x_test = (x_test - 255.) / 255.\n",
    "\n",
    "# reshape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T00:40:30.192118Z",
     "start_time": "2020-06-17T00:40:30.172132Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "hNR6wWrHvMhm",
    "outputId": "14eaba27-6b20-4a22-c3d9-e527a50c8965"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 10)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# y onehotencoder\n",
    "CLASS_NUMS = 10\n",
    "\n",
    "# one-hot y-data\n",
    "y_train = tf.keras.utils.to_categorical(y_train, CLASS_NUMS)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, CLASS_NUMS)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n3m1UdgG09H9"
   },
   "source": [
    "## Model\n",
    "- AlexNet(5 CNN + 3 Dense)\n",
    "    - ReLU\n",
    "    - Dropout\n",
    "    - BN (optional)\n",
    "    - Flatten or GAP2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T00:41:13.246119Z",
     "start_time": "2020-06-17T00:41:13.150377Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 737
    },
    "colab_type": "code",
    "id": "cU64XUdpvMhs",
    "outputId": "7599dfeb-e607-4f93-906b-7f4954a477c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_33 (Conv2D)           (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 32, 32, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_36 (Conv2D)           (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_37 (Conv2D)           (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               2097280   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 2,209,706\n",
      "Trainable params: 2,209,194\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# AlexNet(5 CNN + 3 Dense) + Flatten\n",
    "\n",
    "alexnet = Sequential()\n",
    "\n",
    "# 5 CNN\n",
    "alexnet.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=(32, 32, 3), activation='relu'))\n",
    "alexnet.add(BatchNormalization())           \n",
    "alexnet.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 activation='relu'))   \n",
    "alexnet.add(BatchNormalization())           \n",
    "alexnet.add(Conv2D(64, (3, 3), padding='same',\n",
    "                 activation='relu'))   \n",
    "alexnet.add(BatchNormalization())           \n",
    "alexnet.add(Conv2D(64, (3, 3), padding='same',\n",
    "                 activation='relu')) \n",
    "alexnet.add(BatchNormalization())           \n",
    "alexnet.add(Conv2D(64, (3, 3), padding='same',\n",
    "                 activation='relu')) \n",
    "alexnet.add(BatchNormalization())           \n",
    "alexnet.add(MaxPooling2D())\n",
    "\n",
    "# 3 Dense\n",
    "\n",
    "alexnet.add(Flatten())\n",
    "alexnet.add(Dense(128, activation='relu'))\n",
    "alexnet.add(Dropout(rate=0.2))\n",
    "alexnet.add(Dense(64, activation='relu'))\n",
    "alexnet.add(Dropout(rate=0.2))\n",
    "alexnet.add(Dense(10, activation='softmax'))\n",
    "alexnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T00:43:46.540935Z",
     "start_time": "2020-06-17T00:43:46.535948Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "KN8BwbqtvMhz"
   },
   "outputs": [],
   "source": [
    "# 超參數\n",
    "\n",
    "EPOCHS = 10             # 全部樣本丟進去跑10次\n",
    "BATCH_SIZE = 128        # 一次丟128個樣本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T00:41:46.259874Z",
     "start_time": "2020-06-17T00:41:14.225138Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "id": "XdBW2WfhvMh5",
    "outputId": "8e278ef6-b451-4ba4-fc5e-feb884d64c9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 1.8712 - acc: 0.3423 - val_loss: 1.6213 - val_acc: 0.4212\n",
      "Epoch 2/10\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 1.3386 - acc: 0.5213 - val_loss: 1.2605 - val_acc: 0.5606\n",
      "Epoch 3/10\n",
      "391/391 [==============================] - 15s 37ms/step - loss: 1.0939 - acc: 0.6179 - val_loss: 1.0144 - val_acc: 0.6551\n",
      "Epoch 4/10\n",
      "391/391 [==============================] - 14s 37ms/step - loss: 0.9398 - acc: 0.6747 - val_loss: 1.0168 - val_acc: 0.6473\n",
      "Epoch 5/10\n",
      "391/391 [==============================] - 14s 37ms/step - loss: 0.8256 - acc: 0.7154 - val_loss: 0.9394 - val_acc: 0.6741\n",
      "Epoch 6/10\n",
      "391/391 [==============================] - 14s 37ms/step - loss: 0.7295 - acc: 0.7460 - val_loss: 0.7993 - val_acc: 0.7294\n",
      "Epoch 7/10\n",
      "391/391 [==============================] - 15s 37ms/step - loss: 0.6403 - acc: 0.7788 - val_loss: 0.7934 - val_acc: 0.7327\n",
      "Epoch 8/10\n",
      "391/391 [==============================] - 15s 37ms/step - loss: 0.5681 - acc: 0.8027 - val_loss: 0.9921 - val_acc: 0.6919\n",
      "Epoch 9/10\n",
      "391/391 [==============================] - 15s 37ms/step - loss: 0.4915 - acc: 0.8307 - val_loss: 0.8118 - val_acc: 0.7412\n",
      "Epoch 10/10\n",
      "391/391 [==============================] - 14s 37ms/step - loss: 0.4361 - acc: 0.8512 - val_loss: 0.7995 - val_acc: 0.7537\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7faf5cc3f9b0>"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alexnet.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "alexnet.fit(x_train, y_train,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wmeUfDniaOkx"
   },
   "outputs": [],
   "source": [
    "def build_alexnet(filters=[32, 32, 64, 64, 64], denses=[128, 64], flatten=True, input_shape=(32, 32, 3), classes=10, bn=True, dropout=True):\n",
    "    from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Flatten, Dense, Dropout\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    \n",
    "    alexnet = Sequential()\n",
    "    # 5 CNN\n",
    "    for e in filters:\n",
    "        alexnet.add(Conv2D(e, (3, 3), padding='same',\n",
    "                           input_shape=input_shape, activation='relu'))\n",
    "        if bn:\n",
    "            alexnet.add(BatchNormalization())       \n",
    "    alexnet.add(MaxPooling2D())\n",
    "\n",
    "    # 3 Dense\n",
    "    if flatten:\n",
    "        alexnet.add(Flatten())\n",
    "    else:\n",
    "        alexnet.add(GlobalAveragePooling2D())\n",
    "    alexnet.add(Dense(denses[0], activation='relu'))\n",
    "    if dropout:\n",
    "        alexnet.add(Dropout(rate=0.2))  \n",
    "    alexnet.add(Dense(denses[1], activation='relu'))\n",
    "    if dropout:\n",
    "        alexnet.add(Dropout(rate=0.2))\n",
    "    alexnet.add(Dense(classes, activation='softmax'))\n",
    "    alexnet.summary()\n",
    "    return alexnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 737
    },
    "colab_type": "code",
    "id": "tcOo_qcncqkB",
    "outputId": "1ec7ab6f-18e1-4837-f9ea-0a5eb053ce5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               2097280   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 2,209,706\n",
      "Trainable params: 2,209,194\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_alexnet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T00:41:46.362598Z",
     "start_time": "2020-06-17T00:41:46.263862Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 737
    },
    "colab_type": "code",
    "id": "KZjdk4p_vMh-",
    "outputId": "b07b0a10-f389-4868-c875-409f43c6830b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_38 (Conv2D)           (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_39 (Conv2D)           (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_40 (Conv2D)           (None, 32, 32, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_41 (Conv2D)           (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_42 (Conv2D)           (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 120,746\n",
      "Trainable params: 120,234\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# AlexNet + GlobalAveragePooling\n",
    "\n",
    "\n",
    "alexnet_gap = Sequential()\n",
    "\n",
    "\n",
    "# 5 CNN\n",
    "alexnet_gap.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=(32, 32, 3), activation='relu'))\n",
    "alexnet_gap.add(BatchNormalization())\n",
    "alexnet_gap.add(Conv2D(32, (3, 3), padding='same', activation='relu'))\n",
    "alexnet_gap.add(BatchNormalization())\n",
    "alexnet_gap.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "alexnet_gap.add(BatchNormalization())\n",
    "alexnet_gap.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "alexnet_gap.add(BatchNormalization())\n",
    "alexnet_gap.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "alexnet_gap.add(BatchNormalization())\n",
    "alexnet_gap.add(MaxPooling2D())\n",
    "\n",
    "# 3 Dense\n",
    "alexnet_gap.add(GlobalAveragePooling2D())\n",
    "alexnet_gap.add(Dense(128, activation='relu'))\n",
    "alexnet_gap.add(Dropout(rate=0.2))\n",
    "alexnet_gap.add(Dense(64, activation='relu'))\n",
    "alexnet_gap.add(Dropout(rate=0.2))\n",
    "alexnet_gap.add(Dense(10, activation='softmax'))\n",
    "alexnet_gap.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "id": "VgQwHtPtvMiF",
    "outputId": "384d0fe0-6ffa-429d-fa96-759cc685b3f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "391/391 [==============================] - 14s 37ms/step - loss: 1.5926 - acc: 0.4075 - val_loss: 3.6845 - val_acc: 0.1655\n",
      "Epoch 2/10\n",
      "391/391 [==============================] - 14s 37ms/step - loss: 1.1898 - acc: 0.5718 - val_loss: 1.2981 - val_acc: 0.5315\n",
      "Epoch 3/10\n",
      "391/391 [==============================] - 14s 37ms/step - loss: 1.0179 - acc: 0.6383 - val_loss: 1.0481 - val_acc: 0.6236\n",
      "Epoch 4/10\n",
      "391/391 [==============================] - 14s 37ms/step - loss: 0.9148 - acc: 0.6739 - val_loss: 1.1074 - val_acc: 0.5943\n",
      "Epoch 5/10\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.8448 - acc: 0.7018 - val_loss: 0.9443 - val_acc: 0.6662\n",
      "Epoch 6/10\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.7822 - acc: 0.7250 - val_loss: 0.9526 - val_acc: 0.6620\n",
      "Epoch 7/10\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.7324 - acc: 0.7450 - val_loss: 1.0004 - val_acc: 0.6679\n",
      "Epoch 8/10\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.6861 - acc: 0.7613 - val_loss: 0.8064 - val_acc: 0.7325\n",
      "Epoch 9/10\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.6401 - acc: 0.7789 - val_loss: 0.8951 - val_acc: 0.6942\n",
      "Epoch 10/10\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.6036 - acc: 0.7918 - val_loss: 1.0311 - val_acc: 0.6682\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7faf5c9af2e8>"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alexnet_gap.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "alexnet_gap.fit(x_train, y_train,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "noIkojtN3-c_"
   },
   "source": [
    "## 根據上面的基本AlexNet 去改進\n",
    "- filter size變大\n",
    "    - 相當於特徵擷取變多\n",
    "    - 結果差不多, 但看起來不太穩定\n",
    "- Epochs增加(原先10)\n",
    "    - 增加為20\n",
    "        - 沒有太大改變\n",
    "- Dense神經元增加(原先128, 64)\n",
    "    - 增加為256, 128\n",
    "        - 沒什麼用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 737
    },
    "colab_type": "code",
    "id": "Qx53m9S_vMiK",
    "outputId": "580dd5ec-8e2c-464c-f523-a7b390c5289c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"AlexNet3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_53 (Conv2D)           (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_54 (Conv2D)           (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_55 (Conv2D)           (None, 32, 32, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_56 (Conv2D)           (None, 32, 32, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_57 (Conv2D)           (None, 32, 32, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 128)               4194432   \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 4,613,130\n",
      "Trainable params: 4,612,106\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# filter 變大一些\n",
    "# 原先32, 32, 64, 64, 64\n",
    "# 改變成64, 64, 128, 128, 128\n",
    "\n",
    "alexnet3 = Sequential(name='AlexNet3')\n",
    "\n",
    "\n",
    "# CNN\n",
    "alexnet3.add(Conv2D(64, (3, 3), padding='same', activation='relu', input_shape=x_train.shape[1:]))\n",
    "alexnet3.add(BatchNormalization())\n",
    "alexnet3.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "alexnet3.add(BatchNormalization())\n",
    "alexnet3.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "alexnet3.add(BatchNormalization())\n",
    "alexnet3.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "alexnet3.add(BatchNormalization())\n",
    "alexnet3.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "alexnet3.add(BatchNormalization())\n",
    "alexnet3.add(MaxPooling2D())\n",
    "\n",
    "# Dense\n",
    "alexnet3.add(Flatten())\n",
    "alexnet3.add(Dense(128, activation='relu'))\n",
    "alexnet3.add(Dropout(0.2))\n",
    "alexnet3.add(Dense(64, activation='relu'))\n",
    "alexnet3.add(Dropout(0.2))\n",
    "alexnet3.add(Dense(10, activation='softmax'))\n",
    "\n",
    "alexnet3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "id": "YG71xYFK5cd4",
    "outputId": "e6af9c98-69d5-4298-c9cc-79baa88a56b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 2.0688 - acc: 0.2783 - val_loss: 2.2970 - val_acc: 0.2218\n",
      "Epoch 2/10\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 1.5438 - acc: 0.4383 - val_loss: 1.5009 - val_acc: 0.5037\n",
      "Epoch 3/10\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 1.3101 - acc: 0.5257 - val_loss: 1.1415 - val_acc: 0.6061\n",
      "Epoch 4/10\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 1.1716 - acc: 0.5804 - val_loss: 1.4407 - val_acc: 0.5429\n",
      "Epoch 5/10\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 1.0541 - acc: 0.6245 - val_loss: 1.0129 - val_acc: 0.6550\n",
      "Epoch 6/10\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.9474 - acc: 0.6593 - val_loss: 1.0093 - val_acc: 0.6580\n",
      "Epoch 7/10\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.8432 - acc: 0.7012 - val_loss: 0.8652 - val_acc: 0.7089\n",
      "Epoch 8/10\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.7237 - acc: 0.7492 - val_loss: 1.1704 - val_acc: 0.6320\n",
      "Epoch 9/10\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.6342 - acc: 0.7818 - val_loss: 0.9812 - val_acc: 0.6785\n",
      "Epoch 10/10\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.5457 - acc: 0.8105 - val_loss: 0.7122 - val_acc: 0.7604\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7faf525361d0>"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "alexnet3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "alexnet3.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "5OFbV1W-6KLM",
    "outputId": "acc08ef6-2c15-4583-8ba6-935c9c9e89d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"AlexNet4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_68 (Conv2D)           (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "batch_normalization_35 (Batc (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_69 (Conv2D)           (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_36 (Batc (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_70 (Conv2D)           (None, 32, 32, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_37 (Batc (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_71 (Conv2D)           (None, 32, 32, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_38 (Batc (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_72 (Conv2D)           (None, 32, 32, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_39 (Batc (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_3 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 435,210\n",
      "Trainable params: 434,186\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 1.5727 - acc: 0.4150 - val_loss: 4.2544 - val_acc: 0.1275\n",
      "Epoch 2/10\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 1.1491 - acc: 0.5876 - val_loss: 1.4109 - val_acc: 0.4972\n",
      "Epoch 3/10\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.9505 - acc: 0.6650 - val_loss: 1.2966 - val_acc: 0.5689\n",
      "Epoch 4/10\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.8317 - acc: 0.7103 - val_loss: 0.9619 - val_acc: 0.6541\n",
      "Epoch 5/10\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.7313 - acc: 0.7479 - val_loss: 0.9730 - val_acc: 0.6667\n",
      "Epoch 6/10\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.6523 - acc: 0.7764 - val_loss: 1.0743 - val_acc: 0.6306\n",
      "Epoch 7/10\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.5840 - acc: 0.7999 - val_loss: 0.9279 - val_acc: 0.6760\n",
      "Epoch 8/10\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.5226 - acc: 0.8205 - val_loss: 0.7718 - val_acc: 0.7462\n",
      "Epoch 9/10\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.4640 - acc: 0.8439 - val_loss: 0.6963 - val_acc: 0.7690\n",
      "Epoch 10/10\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.4174 - acc: 0.8578 - val_loss: 0.7009 - val_acc: 0.7708\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7faebc7d3390>"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alexnet4 = Sequential(name='AlexNet4')\n",
    "\n",
    "\n",
    "# CNN\n",
    "alexnet4.add(Conv2D(64, (3, 3), padding='same', activation='relu', input_shape=x_train.shape[1:]))\n",
    "alexnet4.add(BatchNormalization())\n",
    "alexnet4.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "alexnet4.add(BatchNormalization())\n",
    "alexnet4.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "alexnet4.add(BatchNormalization())\n",
    "alexnet4.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "alexnet4.add(BatchNormalization())\n",
    "alexnet4.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "alexnet4.add(BatchNormalization())\n",
    "alexnet4.add(MaxPooling2D())\n",
    "\n",
    "# Dense\n",
    "alexnet4.add(GlobalAveragePooling2D())\n",
    "alexnet4.add(Dense(128, activation='relu'))\n",
    "alexnet4.add(Dropout(0.2))\n",
    "alexnet4.add(Dense(64, activation='relu'))\n",
    "alexnet4.add(Dropout(0.2))\n",
    "alexnet4.add(Dense(10, activation='softmax'))\n",
    "\n",
    "alexnet4.summary()\n",
    "\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "alexnet4.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "alexnet4.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "vfafW7PL8_Ji",
    "outputId": "058893be-b555-46a0-f6db-c8e89c199b1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 32, 32, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               2097280   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 2,209,706\n",
      "Trainable params: 2,209,194\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 1.8192 - acc: 0.3619 - val_loss: 1.9347 - val_acc: 0.3241\n",
      "Epoch 2/20\n",
      "391/391 [==============================] - 17s 43ms/step - loss: 1.2891 - acc: 0.5527 - val_loss: 1.1797 - val_acc: 0.5931\n",
      "Epoch 3/20\n",
      "391/391 [==============================] - 17s 42ms/step - loss: 1.0550 - acc: 0.6349 - val_loss: 0.9910 - val_acc: 0.6588\n",
      "Epoch 4/20\n",
      "391/391 [==============================] - 17s 42ms/step - loss: 0.9208 - acc: 0.6846 - val_loss: 1.1125 - val_acc: 0.6336\n",
      "Epoch 5/20\n",
      "391/391 [==============================] - 17s 42ms/step - loss: 0.8174 - acc: 0.7205 - val_loss: 0.8407 - val_acc: 0.7190\n",
      "Epoch 6/20\n",
      "391/391 [==============================] - 17s 43ms/step - loss: 0.7322 - acc: 0.7497 - val_loss: 0.8004 - val_acc: 0.7319\n",
      "Epoch 7/20\n",
      "391/391 [==============================] - 17s 42ms/step - loss: 0.6442 - acc: 0.7779 - val_loss: 0.8298 - val_acc: 0.7261\n",
      "Epoch 8/20\n",
      "391/391 [==============================] - 17s 42ms/step - loss: 0.5783 - acc: 0.8002 - val_loss: 0.8322 - val_acc: 0.7295\n",
      "Epoch 9/20\n",
      "391/391 [==============================] - 17s 43ms/step - loss: 0.5122 - acc: 0.8243 - val_loss: 0.8595 - val_acc: 0.7278\n",
      "Epoch 10/20\n",
      "391/391 [==============================] - 17s 42ms/step - loss: 0.4599 - acc: 0.8414 - val_loss: 0.7567 - val_acc: 0.7560\n",
      "Epoch 11/20\n",
      "391/391 [==============================] - 17s 42ms/step - loss: 0.4142 - acc: 0.8582 - val_loss: 0.8263 - val_acc: 0.7411\n",
      "Epoch 12/20\n",
      "391/391 [==============================] - 17s 43ms/step - loss: 0.3804 - acc: 0.8698 - val_loss: 0.8782 - val_acc: 0.7472\n",
      "Epoch 13/20\n",
      "391/391 [==============================] - 17s 42ms/step - loss: 0.3497 - acc: 0.8798 - val_loss: 0.8465 - val_acc: 0.7456\n",
      "Epoch 14/20\n",
      "391/391 [==============================] - 17s 42ms/step - loss: 0.3206 - acc: 0.8910 - val_loss: 0.9113 - val_acc: 0.7403\n",
      "Epoch 15/20\n",
      "391/391 [==============================] - 17s 43ms/step - loss: 0.2981 - acc: 0.8977 - val_loss: 0.8603 - val_acc: 0.7499\n",
      "Epoch 16/20\n",
      "391/391 [==============================] - 17s 42ms/step - loss: 0.2847 - acc: 0.9024 - val_loss: 0.8906 - val_acc: 0.7560\n",
      "Epoch 17/20\n",
      "391/391 [==============================] - 17s 42ms/step - loss: 0.2620 - acc: 0.9111 - val_loss: 1.0142 - val_acc: 0.7437\n",
      "Epoch 18/20\n",
      "391/391 [==============================] - 17s 42ms/step - loss: 0.2531 - acc: 0.9136 - val_loss: 0.9415 - val_acc: 0.7548\n",
      "Epoch 19/20\n",
      "391/391 [==============================] - 17s 42ms/step - loss: 0.2423 - acc: 0.9195 - val_loss: 0.9789 - val_acc: 0.7584\n",
      "Epoch 20/20\n",
      "391/391 [==============================] - 17s 42ms/step - loss: 0.2326 - acc: 0.9221 - val_loss: 0.9621 - val_acc: 0.7574\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f4d94ee8358>"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## epochs 改變\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "model_20 = build_alexnet()\n",
    "\n",
    "model_20.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model_20.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0vwlJBUMf1oi"
   },
   "source": [
    "## 神經元增加\n",
    "- 效果不好\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Ff3pztaOdtrI",
    "outputId": "a563777f-6dc4-4bc9-e2b0-2c17d2c99b06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_10 (Conv2D)           (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 32, 32, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 256)               4194560   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 4,332,266\n",
      "Trainable params: 4,331,754\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "391/391 [==============================] - 17s 43ms/step - loss: 1.7236 - acc: 0.4057 - val_loss: 2.7607 - val_acc: 0.2413\n",
      "Epoch 2/10\n",
      "391/391 [==============================] - 17s 43ms/step - loss: 1.1697 - acc: 0.5873 - val_loss: 1.0667 - val_acc: 0.6314\n",
      "Epoch 3/10\n",
      "391/391 [==============================] - 17s 43ms/step - loss: 0.9455 - acc: 0.6698 - val_loss: 0.8816 - val_acc: 0.6960\n",
      "Epoch 4/10\n",
      "391/391 [==============================] - 17s 43ms/step - loss: 0.7969 - acc: 0.7239 - val_loss: 0.9500 - val_acc: 0.6693\n",
      "Epoch 5/10\n",
      "391/391 [==============================] - 17s 43ms/step - loss: 0.6804 - acc: 0.7640 - val_loss: 0.8085 - val_acc: 0.7253\n",
      "Epoch 6/10\n",
      "391/391 [==============================] - 17s 43ms/step - loss: 0.5753 - acc: 0.8006 - val_loss: 0.9317 - val_acc: 0.7002\n",
      "Epoch 7/10\n",
      "391/391 [==============================] - 17s 43ms/step - loss: 0.4731 - acc: 0.8390 - val_loss: 0.9026 - val_acc: 0.7069\n",
      "Epoch 8/10\n",
      "391/391 [==============================] - 17s 43ms/step - loss: 0.3878 - acc: 0.8660 - val_loss: 0.8463 - val_acc: 0.7305\n",
      "Epoch 9/10\n",
      "391/391 [==============================] - 17s 43ms/step - loss: 0.3191 - acc: 0.8898 - val_loss: 0.9282 - val_acc: 0.7363\n",
      "Epoch 10/10\n",
      "391/391 [==============================] - 17s 43ms/step - loss: 0.2732 - acc: 0.9062 - val_loss: 0.9174 - val_acc: 0.7358\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f4d9425fa90>"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 神經元 \n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "model_256 = build_alexnet(denses=[256, 128])\n",
    "\n",
    "model_256.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model_256.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "wjZNh7XogFnn",
    "outputId": "5fc2aa94-6292-4730-ffe5-5ebd5a4b3791"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "Using real-time data augmentation.\n",
      "Epoch 1/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.8731 - accuracy: 0.3123 - val_loss: 1.6144 - val_accuracy: 0.4143\n",
      "Epoch 2/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.5823 - accuracy: 0.4204 - val_loss: 1.4332 - val_accuracy: 0.4787\n",
      "Epoch 3/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.4492 - accuracy: 0.4759 - val_loss: 1.3385 - val_accuracy: 0.5137\n",
      "Epoch 4/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.3705 - accuracy: 0.5054 - val_loss: 1.2200 - val_accuracy: 0.5631\n",
      "Epoch 5/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.3019 - accuracy: 0.5339 - val_loss: 1.2006 - val_accuracy: 0.5720\n",
      "Epoch 6/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.2486 - accuracy: 0.5551 - val_loss: 1.2184 - val_accuracy: 0.5713\n",
      "Epoch 7/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.1936 - accuracy: 0.5760 - val_loss: 1.0566 - val_accuracy: 0.6268\n",
      "Epoch 8/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.1510 - accuracy: 0.5931 - val_loss: 1.0411 - val_accuracy: 0.6359\n",
      "Epoch 9/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.1118 - accuracy: 0.6084 - val_loss: 0.9949 - val_accuracy: 0.6488\n",
      "Epoch 10/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.0823 - accuracy: 0.6176 - val_loss: 1.0545 - val_accuracy: 0.6357\n",
      "Epoch 11/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.0579 - accuracy: 0.6285 - val_loss: 0.9274 - val_accuracy: 0.6752\n",
      "Epoch 12/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.0254 - accuracy: 0.6411 - val_loss: 0.9892 - val_accuracy: 0.6604\n",
      "Epoch 13/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.0044 - accuracy: 0.6470 - val_loss: 0.9053 - val_accuracy: 0.6879\n",
      "Epoch 14/100\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.9874 - accuracy: 0.6520 - val_loss: 0.8393 - val_accuracy: 0.7048\n",
      "Epoch 15/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.9657 - accuracy: 0.6615 - val_loss: 0.9284 - val_accuracy: 0.6874\n",
      "Epoch 16/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.9525 - accuracy: 0.6677 - val_loss: 0.9393 - val_accuracy: 0.6851\n",
      "Epoch 17/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.9362 - accuracy: 0.6716 - val_loss: 0.7930 - val_accuracy: 0.7267\n",
      "Epoch 18/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.9207 - accuracy: 0.6782 - val_loss: 0.7994 - val_accuracy: 0.7214\n",
      "Epoch 19/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.9103 - accuracy: 0.6817 - val_loss: 0.7593 - val_accuracy: 0.7335\n",
      "Epoch 20/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.9001 - accuracy: 0.6868 - val_loss: 0.8086 - val_accuracy: 0.7180\n",
      "Epoch 21/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.8809 - accuracy: 0.6911 - val_loss: 0.8507 - val_accuracy: 0.7052\n",
      "Epoch 22/100\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.8804 - accuracy: 0.6949 - val_loss: 0.7572 - val_accuracy: 0.7350\n",
      "Epoch 23/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.8673 - accuracy: 0.6988 - val_loss: 0.7460 - val_accuracy: 0.7392\n",
      "Epoch 24/100\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.8607 - accuracy: 0.7020 - val_loss: 0.7840 - val_accuracy: 0.7325\n",
      "Epoch 25/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.8530 - accuracy: 0.7057 - val_loss: 0.7550 - val_accuracy: 0.7385\n",
      "Epoch 26/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.8488 - accuracy: 0.7074 - val_loss: 0.7520 - val_accuracy: 0.7414\n",
      "Epoch 27/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.8405 - accuracy: 0.7089 - val_loss: 0.7432 - val_accuracy: 0.7437\n",
      "Epoch 28/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.8387 - accuracy: 0.7130 - val_loss: 0.8069 - val_accuracy: 0.7285\n",
      "Epoch 29/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.8305 - accuracy: 0.7145 - val_loss: 0.7284 - val_accuracy: 0.7447\n",
      "Epoch 30/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.8267 - accuracy: 0.7157 - val_loss: 0.7229 - val_accuracy: 0.7515\n",
      "Epoch 31/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.8247 - accuracy: 0.7170 - val_loss: 0.7555 - val_accuracy: 0.7411\n",
      "Epoch 32/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.8182 - accuracy: 0.7193 - val_loss: 0.7248 - val_accuracy: 0.7536\n",
      "Epoch 33/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.8178 - accuracy: 0.7191 - val_loss: 0.6956 - val_accuracy: 0.7598\n",
      "Epoch 34/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.8230 - accuracy: 0.7183 - val_loss: 0.6935 - val_accuracy: 0.7627\n",
      "Epoch 35/100\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.8141 - accuracy: 0.7230 - val_loss: 0.7261 - val_accuracy: 0.7557\n",
      "Epoch 36/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.8114 - accuracy: 0.7242 - val_loss: 0.7251 - val_accuracy: 0.7528\n",
      "Epoch 37/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.8070 - accuracy: 0.7251 - val_loss: 0.7287 - val_accuracy: 0.7458\n",
      "Epoch 38/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.8084 - accuracy: 0.7245 - val_loss: 0.7354 - val_accuracy: 0.7461\n",
      "Epoch 39/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.8022 - accuracy: 0.7257 - val_loss: 0.7350 - val_accuracy: 0.7522\n",
      "Epoch 40/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.8030 - accuracy: 0.7290 - val_loss: 0.6964 - val_accuracy: 0.7632\n",
      "Epoch 41/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7983 - accuracy: 0.7283 - val_loss: 0.7353 - val_accuracy: 0.7527\n",
      "Epoch 42/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7996 - accuracy: 0.7304 - val_loss: 0.7849 - val_accuracy: 0.7484\n",
      "Epoch 43/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7975 - accuracy: 0.7282 - val_loss: 0.7086 - val_accuracy: 0.7611\n",
      "Epoch 44/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7979 - accuracy: 0.7308 - val_loss: 0.7006 - val_accuracy: 0.7658\n",
      "Epoch 45/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7962 - accuracy: 0.7325 - val_loss: 0.6845 - val_accuracy: 0.7702\n",
      "Epoch 46/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7943 - accuracy: 0.7332 - val_loss: 0.6942 - val_accuracy: 0.7637\n",
      "Epoch 47/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7962 - accuracy: 0.7303 - val_loss: 0.7318 - val_accuracy: 0.7528\n",
      "Epoch 48/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7874 - accuracy: 0.7331 - val_loss: 0.6636 - val_accuracy: 0.7797\n",
      "Epoch 49/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7897 - accuracy: 0.7333 - val_loss: 0.7090 - val_accuracy: 0.7597\n",
      "Epoch 50/100\n",
      "1563/1563 [==============================] - 30s 20ms/step - loss: 0.7877 - accuracy: 0.7358 - val_loss: 0.7593 - val_accuracy: 0.7540\n",
      "Epoch 51/100\n",
      "1563/1563 [==============================] - 30s 20ms/step - loss: 0.7868 - accuracy: 0.7347 - val_loss: 0.7070 - val_accuracy: 0.7631\n",
      "Epoch 52/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7843 - accuracy: 0.7351 - val_loss: 0.7165 - val_accuracy: 0.7521\n",
      "Epoch 53/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7861 - accuracy: 0.7336 - val_loss: 0.6712 - val_accuracy: 0.7735\n",
      "Epoch 54/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7853 - accuracy: 0.7348 - val_loss: 0.6834 - val_accuracy: 0.7698\n",
      "Epoch 55/100\n",
      "1563/1563 [==============================] - 30s 20ms/step - loss: 0.7859 - accuracy: 0.7346 - val_loss: 0.6992 - val_accuracy: 0.7657\n",
      "Epoch 56/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7854 - accuracy: 0.7348 - val_loss: 0.7060 - val_accuracy: 0.7610\n",
      "Epoch 57/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7864 - accuracy: 0.7359 - val_loss: 0.7231 - val_accuracy: 0.7529\n",
      "Epoch 58/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7815 - accuracy: 0.7364 - val_loss: 0.6943 - val_accuracy: 0.7656\n",
      "Epoch 59/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7835 - accuracy: 0.7383 - val_loss: 0.6717 - val_accuracy: 0.7771\n",
      "Epoch 60/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7870 - accuracy: 0.7335 - val_loss: 0.6725 - val_accuracy: 0.7784\n",
      "Epoch 61/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7867 - accuracy: 0.7361 - val_loss: 0.7006 - val_accuracy: 0.7618\n",
      "Epoch 62/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7813 - accuracy: 0.7391 - val_loss: 0.6896 - val_accuracy: 0.7728\n",
      "Epoch 63/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7809 - accuracy: 0.7402 - val_loss: 0.7095 - val_accuracy: 0.7588\n",
      "Epoch 64/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7791 - accuracy: 0.7385 - val_loss: 0.7022 - val_accuracy: 0.7605\n",
      "Epoch 65/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7794 - accuracy: 0.7378 - val_loss: 0.6858 - val_accuracy: 0.7716\n",
      "Epoch 66/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7837 - accuracy: 0.7410 - val_loss: 0.7287 - val_accuracy: 0.7530\n",
      "Epoch 67/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7872 - accuracy: 0.7364 - val_loss: 0.7133 - val_accuracy: 0.7551\n",
      "Epoch 68/100\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7872 - accuracy: 0.7367 - val_loss: 0.7000 - val_accuracy: 0.7712\n",
      "Epoch 69/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7809 - accuracy: 0.7386 - val_loss: 0.7025 - val_accuracy: 0.7718\n",
      "Epoch 70/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7772 - accuracy: 0.7407 - val_loss: 0.6946 - val_accuracy: 0.7649\n",
      "Epoch 71/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7843 - accuracy: 0.7374 - val_loss: 0.6815 - val_accuracy: 0.7716\n",
      "Epoch 72/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7810 - accuracy: 0.7402 - val_loss: 0.6773 - val_accuracy: 0.7708\n",
      "Epoch 73/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7862 - accuracy: 0.7398 - val_loss: 0.7490 - val_accuracy: 0.7458\n",
      "Epoch 74/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7844 - accuracy: 0.7371 - val_loss: 0.6941 - val_accuracy: 0.7660\n",
      "Epoch 75/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7876 - accuracy: 0.7391 - val_loss: 0.6965 - val_accuracy: 0.7848\n",
      "Epoch 76/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7853 - accuracy: 0.7376 - val_loss: 0.7481 - val_accuracy: 0.7478\n",
      "Epoch 77/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7864 - accuracy: 0.7385 - val_loss: 0.6736 - val_accuracy: 0.7748\n",
      "Epoch 78/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7835 - accuracy: 0.7401 - val_loss: 0.7062 - val_accuracy: 0.7629\n",
      "Epoch 79/100\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7897 - accuracy: 0.7381 - val_loss: 0.7444 - val_accuracy: 0.7495\n",
      "Epoch 80/100\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7866 - accuracy: 0.7381 - val_loss: 0.7002 - val_accuracy: 0.7685\n",
      "Epoch 81/100\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7839 - accuracy: 0.7392 - val_loss: 0.6754 - val_accuracy: 0.7751\n",
      "Epoch 82/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7879 - accuracy: 0.7387 - val_loss: 0.7366 - val_accuracy: 0.7507\n",
      "Epoch 83/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7855 - accuracy: 0.7417 - val_loss: 0.6599 - val_accuracy: 0.7820\n",
      "Epoch 84/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7897 - accuracy: 0.7375 - val_loss: 0.7038 - val_accuracy: 0.7684\n",
      "Epoch 85/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7863 - accuracy: 0.7384 - val_loss: 0.7098 - val_accuracy: 0.7566\n",
      "Epoch 86/100\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7843 - accuracy: 0.7395 - val_loss: 0.6532 - val_accuracy: 0.7829\n",
      "Epoch 87/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7878 - accuracy: 0.7396 - val_loss: 0.7712 - val_accuracy: 0.7612\n",
      "Epoch 88/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7916 - accuracy: 0.7397 - val_loss: 0.7636 - val_accuracy: 0.7439\n",
      "Epoch 89/100\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7944 - accuracy: 0.7382 - val_loss: 0.7751 - val_accuracy: 0.7440\n",
      "Epoch 90/100\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7918 - accuracy: 0.7355 - val_loss: 0.7876 - val_accuracy: 0.7427\n",
      "Epoch 91/100\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7946 - accuracy: 0.7385 - val_loss: 0.7120 - val_accuracy: 0.7619\n",
      "Epoch 92/100\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7975 - accuracy: 0.7367 - val_loss: 0.7516 - val_accuracy: 0.7513\n",
      "Epoch 93/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7968 - accuracy: 0.7379 - val_loss: 0.7535 - val_accuracy: 0.7535\n",
      "Epoch 94/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7972 - accuracy: 0.7342 - val_loss: 0.6786 - val_accuracy: 0.7823\n",
      "Epoch 95/100\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.8028 - accuracy: 0.7333 - val_loss: 0.7310 - val_accuracy: 0.7632\n",
      "Epoch 96/100\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.8038 - accuracy: 0.7349 - val_loss: 0.6904 - val_accuracy: 0.7787\n",
      "Epoch 97/100\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.8083 - accuracy: 0.7335 - val_loss: 0.6940 - val_accuracy: 0.7690\n",
      "Epoch 98/100\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.8090 - accuracy: 0.7335 - val_loss: 0.7328 - val_accuracy: 0.7510\n",
      "Epoch 99/100\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 0.8082 - accuracy: 0.7353 - val_loss: 0.7299 - val_accuracy: 0.7619\n",
      "Epoch 100/100\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 0.8089 - accuracy: 0.7345 - val_loss: 0.7192 - val_accuracy: 0.7646\n",
      "Saved trained model at /content/saved_models/keras_cifar10_trained_model.h5 \n",
      "10000/10000 [==============================] - 1s 107us/step\n",
      "Test loss: 0.7192324872970581\n",
      "Test accuracy: 0.7645999789237976\n"
     ]
    }
   ],
   "source": [
    "# 官方文檔的教學\n",
    "\n",
    "\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import os\n",
    "\n",
    "batch_size = 32\n",
    "num_classes = 10\n",
    "epochs = 100\n",
    "data_augmentation = True\n",
    "num_predictions = 20\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'keras_cifar10_trained_model.h5'\n",
    "\n",
    "# The data, split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.RMSprop(learning_rate=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        # randomly shift images horizontally (fraction of total width)\n",
    "        width_shift_range=0.1,\n",
    "        # randomly shift images vertically (fraction of total height)\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.,  # set range for random shear\n",
    "        zoom_range=0.,  # set range for random zoom\n",
    "        channel_shift_range=0.,  # set range for random channel shifts\n",
    "        # set mode for filling points outside the input boundaries\n",
    "        fill_mode='nearest',\n",
    "        cval=0.,  # value used for fill_mode = \"constant\"\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False,  # randomly flip images\n",
    "        # set rescaling factor (applied before any other transformation)\n",
    "        rescale=None,\n",
    "        # set function that will be applied on each input\n",
    "        preprocessing_function=None,\n",
    "        # image data format, either \"channels_first\" or \"channels_last\"\n",
    "        data_format=None,\n",
    "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "        validation_split=0.0)\n",
    "\n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                     batch_size=batch_size),\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        workers=4)\n",
    "\n",
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)\n",
    "\n",
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4ap_Vw-7l7oK"
   },
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "gnS90cqshHom",
    "outputId": "bcbb2d45-52e5-4577-be72-bdc2ed257d39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_20 (Conv2D)           (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 32, 32, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 128)               2097280   \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 2,209,706\n",
      "Trainable params: 2,209,194\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 1.9187 - acc: 0.3187 - val_loss: 2.1431 - val_acc: 0.2779\n",
      "Epoch 2/100\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 1.4779 - acc: 0.4666 - val_loss: 1.1768 - val_acc: 0.5952\n",
      "Epoch 3/100\n",
      "391/391 [==============================] - 31s 79ms/step - loss: 1.2669 - acc: 0.5513 - val_loss: 1.1992 - val_acc: 0.5909\n",
      "Epoch 4/100\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 1.1506 - acc: 0.5973 - val_loss: 1.0890 - val_acc: 0.6194\n",
      "Epoch 5/100\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 1.0688 - acc: 0.6304 - val_loss: 1.0317 - val_acc: 0.6494\n",
      "Epoch 6/100\n",
      "391/391 [==============================] - 31s 79ms/step - loss: 0.9895 - acc: 0.6567 - val_loss: 0.9304 - val_acc: 0.6792\n",
      "Epoch 7/100\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.9402 - acc: 0.6768 - val_loss: 0.9893 - val_acc: 0.6662\n",
      "Epoch 8/100\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.8778 - acc: 0.6988 - val_loss: 1.1556 - val_acc: 0.6206\n",
      "Epoch 9/100\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.8374 - acc: 0.7153 - val_loss: 0.8591 - val_acc: 0.7110\n",
      "Epoch 10/100\n",
      "391/391 [==============================] - 31s 79ms/step - loss: 0.8039 - acc: 0.7261 - val_loss: 0.8255 - val_acc: 0.7247\n",
      "Epoch 11/100\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.7733 - acc: 0.7381 - val_loss: 0.6868 - val_acc: 0.7622\n",
      "Epoch 12/100\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.7384 - acc: 0.7500 - val_loss: 0.8110 - val_acc: 0.7245\n",
      "Epoch 13/100\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.7186 - acc: 0.7559 - val_loss: 0.7698 - val_acc: 0.7440\n",
      "Epoch 14/100\n",
      "391/391 [==============================] - 32s 82ms/step - loss: 0.6937 - acc: 0.7653 - val_loss: 0.6969 - val_acc: 0.7651\n",
      "Epoch 15/100\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.6755 - acc: 0.7723 - val_loss: 0.7452 - val_acc: 0.7462\n",
      "Epoch 16/100\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.6599 - acc: 0.7783 - val_loss: 0.6685 - val_acc: 0.7722\n",
      "Epoch 17/100\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.6363 - acc: 0.7851 - val_loss: 0.7049 - val_acc: 0.7606\n",
      "Epoch 18/100\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.6243 - acc: 0.7902 - val_loss: 0.6917 - val_acc: 0.7724\n",
      "Epoch 19/100\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.6095 - acc: 0.7959 - val_loss: 0.6611 - val_acc: 0.7793\n",
      "Epoch 20/100\n",
      "391/391 [==============================] - 31s 79ms/step - loss: 0.6002 - acc: 0.7977 - val_loss: 0.6289 - val_acc: 0.7870\n",
      "Epoch 21/100\n",
      "391/391 [==============================] - 31s 81ms/step - loss: 0.5813 - acc: 0.8035 - val_loss: 0.5886 - val_acc: 0.7979\n",
      "Epoch 22/100\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.5765 - acc: 0.8078 - val_loss: 0.5984 - val_acc: 0.7980\n",
      "Epoch 23/100\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.5606 - acc: 0.8126 - val_loss: 0.6087 - val_acc: 0.8031\n",
      "Epoch 24/100\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.5508 - acc: 0.8162 - val_loss: 0.7106 - val_acc: 0.7609\n",
      "Epoch 25/100\n",
      "391/391 [==============================] - 31s 79ms/step - loss: 0.5376 - acc: 0.8218 - val_loss: 0.5707 - val_acc: 0.8125\n",
      "Epoch 26/100\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.5333 - acc: 0.8231 - val_loss: 0.6737 - val_acc: 0.7796\n",
      "Epoch 27/100\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.5227 - acc: 0.8249 - val_loss: 0.4911 - val_acc: 0.8336\n",
      "Epoch 28/100\n",
      "391/391 [==============================] - 31s 81ms/step - loss: 0.5177 - acc: 0.8280 - val_loss: 0.5684 - val_acc: 0.8104\n",
      "Epoch 29/100\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.5011 - acc: 0.8326 - val_loss: 0.6732 - val_acc: 0.7886\n",
      "Epoch 30/100\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.4956 - acc: 0.8341 - val_loss: 0.6862 - val_acc: 0.7796\n",
      "Epoch 31/100\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.4921 - acc: 0.8368 - val_loss: 0.5452 - val_acc: 0.8241\n",
      "Epoch 32/100\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.4893 - acc: 0.8366 - val_loss: 0.5900 - val_acc: 0.8106\n",
      "Epoch 33/100\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.4733 - acc: 0.8423 - val_loss: 0.5720 - val_acc: 0.8161\n",
      "Epoch 34/100\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.4714 - acc: 0.8425 - val_loss: 0.6001 - val_acc: 0.8008\n",
      "Epoch 35/100\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.4643 - acc: 0.8455 - val_loss: 0.6076 - val_acc: 0.8009\n",
      "Epoch 36/100\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.4571 - acc: 0.8479 - val_loss: 0.5265 - val_acc: 0.8218\n",
      "Epoch 37/100\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.4550 - acc: 0.8476 - val_loss: 0.6198 - val_acc: 0.7996\n",
      "Epoch 38/100\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.4494 - acc: 0.8501 - val_loss: 0.5452 - val_acc: 0.8187\n",
      "Epoch 39/100\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.4399 - acc: 0.8527 - val_loss: 0.5379 - val_acc: 0.8235\n",
      "Epoch 40/100\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.4372 - acc: 0.8543 - val_loss: 0.5183 - val_acc: 0.8279\n",
      "Epoch 41/100\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.4260 - acc: 0.8563 - val_loss: 0.5364 - val_acc: 0.8221\n",
      "Epoch 42/100\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.4240 - acc: 0.8586 - val_loss: 0.5302 - val_acc: 0.8230\n",
      "Epoch 43/100\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.4265 - acc: 0.8581 - val_loss: 0.5652 - val_acc: 0.8183\n",
      "Epoch 44/100\n",
      "391/391 [==============================] - 31s 79ms/step - loss: 0.4172 - acc: 0.8613 - val_loss: 0.5130 - val_acc: 0.8283\n",
      "Epoch 45/100\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.4144 - acc: 0.8629 - val_loss: 0.5223 - val_acc: 0.8311\n",
      "Epoch 46/100\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.4032 - acc: 0.8639 - val_loss: 0.4939 - val_acc: 0.8404\n",
      "Epoch 47/100\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.4005 - acc: 0.8668 - val_loss: 0.5454 - val_acc: 0.8227\n",
      "Epoch 48/100\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.4000 - acc: 0.8682 - val_loss: 0.5568 - val_acc: 0.8216\n",
      "Epoch 49/100\n",
      "391/391 [==============================] - 31s 79ms/step - loss: 0.3983 - acc: 0.8672 - val_loss: 0.6766 - val_acc: 0.7922\n",
      "Epoch 50/100\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.3862 - acc: 0.8707 - val_loss: 0.5355 - val_acc: 0.8316\n",
      "Epoch 51/100\n",
      "391/391 [==============================] - 31s 79ms/step - loss: 0.3905 - acc: 0.8719 - val_loss: 0.5945 - val_acc: 0.8140\n",
      "Epoch 52/100\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.3851 - acc: 0.8720 - val_loss: 0.4994 - val_acc: 0.8374\n",
      "Epoch 53/100\n",
      "391/391 [==============================] - 31s 79ms/step - loss: 0.3784 - acc: 0.8745 - val_loss: 0.5084 - val_acc: 0.8340\n",
      "Epoch 54/100\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.3731 - acc: 0.8762 - val_loss: 0.4882 - val_acc: 0.8415\n",
      "Epoch 55/100\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.3760 - acc: 0.8756 - val_loss: 0.5259 - val_acc: 0.8309\n",
      "Epoch 56/100\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.3702 - acc: 0.8763 - val_loss: 0.5356 - val_acc: 0.8309\n",
      "Epoch 57/100\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.3694 - acc: 0.8775 - val_loss: 0.5759 - val_acc: 0.8220\n",
      "Epoch 58/100\n",
      "391/391 [==============================] - 32s 82ms/step - loss: 0.3603 - acc: 0.8799 - val_loss: 0.5135 - val_acc: 0.8326\n",
      "Epoch 59/100\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.3607 - acc: 0.8785 - val_loss: 0.5561 - val_acc: 0.8307\n",
      "Epoch 60/100\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.3544 - acc: 0.8808 - val_loss: 0.5347 - val_acc: 0.8367\n",
      "Epoch 61/100\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.3595 - acc: 0.8794 - val_loss: 0.5099 - val_acc: 0.8369\n",
      "Epoch 62/100\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.3534 - acc: 0.8835 - val_loss: 0.5303 - val_acc: 0.8313\n",
      "Epoch 63/100\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.3464 - acc: 0.8849 - val_loss: 0.5058 - val_acc: 0.8442\n",
      "Epoch 64/100\n",
      "391/391 [==============================] - 31s 79ms/step - loss: 0.3427 - acc: 0.8864 - val_loss: 0.5964 - val_acc: 0.8176\n",
      "Epoch 65/100\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.3433 - acc: 0.8860 - val_loss: 0.4852 - val_acc: 0.8467\n",
      "Epoch 66/100\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.3392 - acc: 0.8877 - val_loss: 0.5828 - val_acc: 0.8231\n",
      "Epoch 67/100\n",
      "391/391 [==============================] - 31s 79ms/step - loss: 0.3345 - acc: 0.8885 - val_loss: 0.4892 - val_acc: 0.8464\n",
      "Epoch 68/100\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.3342 - acc: 0.8893 - val_loss: 0.5599 - val_acc: 0.8299\n",
      "Epoch 69/100\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.3361 - acc: 0.8886 - val_loss: 0.6152 - val_acc: 0.8203\n",
      "Epoch 70/100\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.3274 - acc: 0.8909 - val_loss: 0.5201 - val_acc: 0.8393\n",
      "Epoch 71/100\n",
      "391/391 [==============================] - 31s 79ms/step - loss: 0.3288 - acc: 0.8903 - val_loss: 0.4824 - val_acc: 0.8477\n",
      "Epoch 72/100\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.3294 - acc: 0.8897 - val_loss: 0.5579 - val_acc: 0.8343\n",
      "Epoch 73/100\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.3266 - acc: 0.8915 - val_loss: 0.6368 - val_acc: 0.8180\n",
      "Epoch 74/100\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.3210 - acc: 0.8917 - val_loss: 0.5085 - val_acc: 0.8452\n",
      "Epoch 75/100\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.3174 - acc: 0.8941 - val_loss: 0.4671 - val_acc: 0.8479\n",
      "Epoch 76/100\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.3184 - acc: 0.8931 - val_loss: 0.5710 - val_acc: 0.8327\n",
      "Epoch 77/100\n",
      "391/391 [==============================] - 32s 82ms/step - loss: 0.3152 - acc: 0.8953 - val_loss: 0.5353 - val_acc: 0.8418\n",
      "Epoch 78/100\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.3144 - acc: 0.8957 - val_loss: 0.5876 - val_acc: 0.8291\n",
      "Epoch 79/100\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.3103 - acc: 0.8963 - val_loss: 0.5252 - val_acc: 0.8398\n",
      "Epoch 80/100\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.3066 - acc: 0.8976 - val_loss: 0.5023 - val_acc: 0.8443\n",
      "Epoch 81/100\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.3135 - acc: 0.8965 - val_loss: 0.5638 - val_acc: 0.8313\n",
      "Epoch 82/100\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.3035 - acc: 0.8995 - val_loss: 0.4746 - val_acc: 0.8493\n",
      "Epoch 83/100\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.3040 - acc: 0.8978 - val_loss: 0.5447 - val_acc: 0.8399\n",
      "Epoch 84/100\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.2978 - acc: 0.9003 - val_loss: 0.4477 - val_acc: 0.8576\n",
      "Epoch 85/100\n",
      "391/391 [==============================] - 32s 82ms/step - loss: 0.2984 - acc: 0.8996 - val_loss: 0.4685 - val_acc: 0.8562\n",
      "Epoch 86/100\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.3020 - acc: 0.8989 - val_loss: 0.4811 - val_acc: 0.8496\n",
      "Epoch 87/100\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.2963 - acc: 0.9010 - val_loss: 0.4798 - val_acc: 0.8517\n",
      "Epoch 88/100\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.2961 - acc: 0.9013 - val_loss: 0.5053 - val_acc: 0.8504\n",
      "Epoch 89/100\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.2969 - acc: 0.9014 - val_loss: 0.5271 - val_acc: 0.8432\n",
      "Epoch 90/100\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.2909 - acc: 0.9037 - val_loss: 0.4589 - val_acc: 0.8588\n",
      "Epoch 91/100\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.2898 - acc: 0.9028 - val_loss: 0.5014 - val_acc: 0.8468\n",
      "Epoch 92/100\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.2858 - acc: 0.9052 - val_loss: 0.4859 - val_acc: 0.8525\n",
      "Epoch 93/100\n",
      "391/391 [==============================] - 32s 82ms/step - loss: 0.2893 - acc: 0.9028 - val_loss: 0.5842 - val_acc: 0.8325\n",
      "Epoch 94/100\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.2882 - acc: 0.9044 - val_loss: 0.4718 - val_acc: 0.8568\n",
      "Epoch 95/100\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.2875 - acc: 0.9045 - val_loss: 0.5299 - val_acc: 0.8388\n",
      "Epoch 96/100\n",
      "391/391 [==============================] - 31s 79ms/step - loss: 0.2814 - acc: 0.9070 - val_loss: 0.4694 - val_acc: 0.8586\n",
      "Epoch 97/100\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.2739 - acc: 0.9095 - val_loss: 0.4874 - val_acc: 0.8578\n",
      "Epoch 98/100\n",
      "391/391 [==============================] - 31s 79ms/step - loss: 0.2750 - acc: 0.9087 - val_loss: 0.4698 - val_acc: 0.8534\n",
      "Epoch 99/100\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.2729 - acc: 0.9084 - val_loss: 0.6464 - val_acc: 0.8214\n",
      "Epoch 100/100\n",
      "391/391 [==============================] - 31s 79ms/step - loss: 0.2799 - acc: 0.9066 - val_loss: 0.4816 - val_acc: 0.8558\n",
      "313/313 [==============================] - 1s 5ms/step - loss: 0.4816 - acc: 0.8558\n",
      "Test loss: 0.48163291811943054\n",
      "Test accuracy: 0.8557999730110168\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# 超參數\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 100\n",
    "\n",
    "\n",
    "# 這是real-time的圖片處理, 所以其實並沒有增加圖片, 但可以BASED ON這樣的方法去類似增加\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,       # 使得輸入資料集平均為0 \n",
    "    samplewise_center=False,        # 使樣本平均為0\n",
    "    featurewise_std_normalization=False,        # \n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False,\n",
    "    zca_epsilon=1e-06,\n",
    "    rotation_range=10,      # 旋轉, 0~180\n",
    "    width_shift_range=0.1,  # 水平移動比例\n",
    "    height_shift_range=0.1, # 垂直移動比例\n",
    "    shear_range=0.,         # random shear\n",
    "    zoom_range=0.,          # random zoom\n",
    "    channel_shift_range=0.,  # set range for random channel shifts\n",
    "    # set mode for filling points outside the input boundaries\n",
    "    fill_mode='nearest',\n",
    "    cval=0.,  # value used for fill_mode = \"constant\"\n",
    "    horizontal_flip=True,  # randomly flip images\n",
    "    vertical_flip=False,  # randomly flip images\n",
    "    # set rescaling factor (applied before any other transformation)\n",
    "    rescale=None,\n",
    "    # set function that will be applied on each input\n",
    "    preprocessing_function=None,\n",
    "    # image data format, either \"channels_first\" or \"channels_last\"\n",
    "    data_format=None,\n",
    "    # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "    validation_split=0.0\n",
    ")\n",
    "\n",
    "datagen.fit(x_train)\n",
    "\n",
    "model_aug_alexnet = build_alexnet()\n",
    "model_aug_alexnet.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model_aug_alexnet.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                             batch_size=BATCH_SIZE),\n",
    "                                epochs=EPOCHS,\n",
    "                                validation_data=(x_test, y_test),\n",
    "                                workers=4)      # workers 進程數量\n",
    "\n",
    "# Score trained model.\n",
    "scores = model_aug_alexnet.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Ge6UN7L7pHOB",
    "outputId": "9f1dbeda-42c2-4235-e790-3ba6c9e4e2fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4096)              67112960  \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                40970     \n",
      "=================================================================\n",
      "Total params: 84,038,762\n",
      "Trainable params: 84,038,250\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From <ipython-input-9-95d9049ba8c7>:46: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/20\n",
      "391/391 [==============================] - 53s 135ms/step - loss: 2.2993 - acc: 0.3866 - val_loss: 7.6153 - val_acc: 0.1898\n",
      "Epoch 2/20\n",
      "391/391 [==============================] - 52s 133ms/step - loss: 1.3671 - acc: 0.5102 - val_loss: 1.4295 - val_acc: 0.5086\n",
      "Epoch 3/20\n",
      "391/391 [==============================] - 52s 134ms/step - loss: 1.2024 - acc: 0.5754 - val_loss: 1.0656 - val_acc: 0.6233\n",
      "Epoch 4/20\n",
      "391/391 [==============================] - 52s 134ms/step - loss: 1.0773 - acc: 0.6206 - val_loss: 1.1564 - val_acc: 0.5967\n",
      "Epoch 5/20\n",
      "391/391 [==============================] - 52s 133ms/step - loss: 0.9937 - acc: 0.6555 - val_loss: 0.9230 - val_acc: 0.6747\n",
      "Epoch 6/20\n",
      "391/391 [==============================] - 52s 134ms/step - loss: 0.9159 - acc: 0.6816 - val_loss: 1.0203 - val_acc: 0.6486\n",
      "Epoch 7/20\n",
      "391/391 [==============================] - 52s 133ms/step - loss: 0.8663 - acc: 0.7015 - val_loss: 0.9119 - val_acc: 0.6885\n",
      "Epoch 8/20\n",
      "391/391 [==============================] - 52s 133ms/step - loss: 0.8239 - acc: 0.7179 - val_loss: 0.8328 - val_acc: 0.7134\n",
      "Epoch 9/20\n",
      "391/391 [==============================] - 52s 133ms/step - loss: 0.7790 - acc: 0.7302 - val_loss: 0.9106 - val_acc: 0.7037\n",
      "Epoch 10/20\n",
      "391/391 [==============================] - 52s 133ms/step - loss: 0.7394 - acc: 0.7476 - val_loss: 0.9263 - val_acc: 0.6946\n",
      "Epoch 11/20\n",
      "391/391 [==============================] - 52s 134ms/step - loss: 0.6976 - acc: 0.7621 - val_loss: 0.7495 - val_acc: 0.7519\n",
      "Epoch 12/20\n",
      "391/391 [==============================] - 52s 134ms/step - loss: 0.6668 - acc: 0.7717 - val_loss: 0.7491 - val_acc: 0.7534\n",
      "Epoch 13/20\n",
      "391/391 [==============================] - 52s 133ms/step - loss: 0.6467 - acc: 0.7797 - val_loss: 0.7143 - val_acc: 0.7644\n",
      "Epoch 14/20\n",
      "391/391 [==============================] - 52s 134ms/step - loss: 0.6148 - acc: 0.7877 - val_loss: 0.6878 - val_acc: 0.7744\n",
      "Epoch 15/20\n",
      "391/391 [==============================] - 52s 133ms/step - loss: 0.6002 - acc: 0.7937 - val_loss: 0.6421 - val_acc: 0.7864\n",
      "Epoch 16/20\n",
      "391/391 [==============================] - 52s 133ms/step - loss: 0.5725 - acc: 0.8046 - val_loss: 0.7090 - val_acc: 0.7646\n",
      "Epoch 17/20\n",
      "391/391 [==============================] - 52s 133ms/step - loss: 0.5494 - acc: 0.8143 - val_loss: 0.7088 - val_acc: 0.7747\n",
      "Epoch 18/20\n",
      "391/391 [==============================] - 52s 133ms/step - loss: 0.5398 - acc: 0.8160 - val_loss: 0.7055 - val_acc: 0.7734\n",
      "Epoch 19/20\n",
      "391/391 [==============================] - 52s 133ms/step - loss: 0.5153 - acc: 0.8238 - val_loss: 0.5675 - val_acc: 0.8088\n",
      "Epoch 20/20\n",
      "391/391 [==============================] - 52s 133ms/step - loss: 0.5052 - acc: 0.8260 - val_loss: 0.6309 - val_acc: 0.7981\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.6309 - acc: 0.7981\n",
      "Test loss: 0.6308820247650146\n",
      "Test accuracy: 0.7980999946594238\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# 超參數\n",
    "# 特地用來與vgg11比較\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 20\n",
    "\n",
    "\n",
    "# 這是real-time的圖片處理, 所以其實並沒有增加圖片, 但可以BASED ON這樣的方法去類似增加\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,       # 使得輸入資料集平均為0 \n",
    "    samplewise_center=False,        # 使樣本平均為0\n",
    "    featurewise_std_normalization=False,        # \n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False,\n",
    "    zca_epsilon=1e-06,\n",
    "    rotation_range=10,      # 旋轉, 0~180\n",
    "    width_shift_range=0.1,  # 水平移動比例\n",
    "    height_shift_range=0.1, # 垂直移動比例\n",
    "    shear_range=0.,         # random shear\n",
    "    zoom_range=0.,          # random zoom\n",
    "    channel_shift_range=0.,  # set range for random channel shifts\n",
    "    # set mode for filling points outside the input boundaries\n",
    "    fill_mode='nearest',\n",
    "    cval=0.,  # value used for fill_mode = \"constant\"\n",
    "    horizontal_flip=True,  # randomly flip images\n",
    "    vertical_flip=False,  # randomly flip images\n",
    "    # set rescaling factor (applied before any other transformation)\n",
    "    rescale=None,\n",
    "    # set function that will be applied on each input\n",
    "    preprocessing_function=None,\n",
    "    # image data format, either \"channels_first\" or \"channels_last\"\n",
    "    data_format=None,\n",
    "    # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "    validation_split=0.0\n",
    ")\n",
    "\n",
    "datagen.fit(x_train)\n",
    "\n",
    "model_aug_alexnet_20 = build_alexnet(denses=[4096, 4096])\n",
    "model_aug_alexnet_20.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model_aug_alexnet_20.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                             batch_size=BATCH_SIZE),\n",
    "                                epochs=EPOCHS,\n",
    "                                validation_data=(x_test, y_test),\n",
    "                                workers=4)      # workers 進程數量\n",
    "\n",
    "# Score trained model.\n",
    "scores = model_aug_alexnet_20.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rX4SA758P51P"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZjkvJcDXo5by"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "AlexNet_cifar10.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
